{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259a1fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed:\n",
    "# !pip install -U ultralytics\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# --- Your base config ---\n",
    "DATA_YAML = \"defect_detection_3d_printing_yolo/data.yaml\"\n",
    "PROJECT_ROOT = \"runs15\"         # all experiments go here\n",
    "EPOCHS = 15\n",
    "IMGSZ = 896\n",
    "\n",
    "# Choose your two models here (the assignment says \"both models\")\n",
    "MODELS = [\n",
    "    \"yolo11n.pt\",\n",
    "]\n",
    "\n",
    "# Sweeps required by assignment\n",
    "LR_MULTS = [1.0, 5.0, 0.2]     # default, 5x, 0.2x\n",
    "BATCHES  = [8, 16, 32]         # batch sweep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa9c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_name(model_ckpt: str, imgsz: int, batch: int, lr_mult: float):\n",
    "    base = Path(model_ckpt).stem  # \"yolo11s\"\n",
    "    lr_tag = f\"lr{lr_mult:g}x\"     # lr1x, lr5x, lr0.2x\n",
    "    return f\"{base}_img{imgsz}_b{batch}_{lr_tag}\"\n",
    "\n",
    "def run_dir(project: str, name: str) -> Path:\n",
    "    return Path(project) / name\n",
    "\n",
    "def load_results_csv(project: str, name: str) -> pd.DataFrame:\n",
    "    csv_path = run_dir(project, name) / \"results.csv\"\n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing results.csv at: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # YOLO logs epochs starting at 0 in the csv typically; we'll create a 1-based epoch column for plotting\n",
    "    if \"epoch\" in df.columns:\n",
    "        df[\"epoch_1based\"] = df[\"epoch\"] + 1\n",
    "    else:\n",
    "        # fallback: assume row index corresponds to epoch\n",
    "        df[\"epoch_1based\"] = df.index + 1\n",
    "    return df\n",
    "\n",
    "def pick_col(df: pd.DataFrame, candidates):\n",
    "    \"\"\"Return the first existing column name from candidates, else None.\"\"\"\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def plot_metric(df: pd.DataFrame, y_col: str, title: str, label: str = None, x_col: str = \"epoch_1based\"):\n",
    "    plt.plot(df[x_col], df[y_col], marker=\"o\", linewidth=1.5, label=label)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "def finalize_plot(ylabel: str, legend=True):\n",
    "    plt.ylabel(ylabel)\n",
    "    if legend:\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cc3683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(model_ckpt: str, imgsz: int, batch: int, lr_mult: float, project: str, epochs: int):\n",
    "    name = run_name(model_ckpt, imgsz, batch, lr_mult)\n",
    "\n",
    "    # Skip if already trained (optional)\n",
    "    out_dir = run_dir(project, name)\n",
    "    if (out_dir / \"results.csv\").exists():\n",
    "        print(f\"[SKIP] {name} already exists.\")\n",
    "        return name\n",
    "\n",
    "    model = YOLO(model_ckpt)\n",
    "\n",
    "    # Ultralytics hyperparams:\n",
    "    # - lr0 is initial learning rate; default depends on model/task, so we get \"default ×\" by not setting it.\n",
    "    # - BUT for 5× and 0.2× we need a baseline. The cleanest way is:\n",
    "    #   1) run default with lr_mult=1.0 (no lr0 override)\n",
    "    #   2) for others, override lr0 relative to a chosen baseline value you define\n",
    "    #\n",
    "    # If your class expects strict \"× default\", then you should set BASE_LR0 to the default used in your training config.\n",
    "    # Common default for YOLO detect is around 0.01, but verify in your training logs.\n",
    "    BASE_LR0 = 0.01\n",
    "\n",
    "    train_kwargs = dict(\n",
    "        data=DATA_YAML,\n",
    "        epochs=epochs,\n",
    "        imgsz=imgsz,\n",
    "        batch=batch,\n",
    "        project=project,\n",
    "        name=name,\n",
    "        # device=0,  # uncomment if you want GPU index\n",
    "        plots=True,  # YOLO will generate built-in plots (results, PR, F1, confusion, etc.)\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    if lr_mult != 1.0:\n",
    "        train_kwargs[\"lr0\"] = BASE_LR0 * lr_mult\n",
    "\n",
    "    print(f\"[TRAIN] {name}\")\n",
    "    model.train(**train_kwargs)\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96b72d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs = []\n",
    "\n",
    "# --- LR sweep at default batch 16 ---\n",
    "for model_ckpt in MODELS:\n",
    "    for lr_mult in LR_MULTS:\n",
    "        name = train_one(\n",
    "            model_ckpt=model_ckpt,\n",
    "            imgsz=IMGSZ,\n",
    "            batch=16,\n",
    "            lr_mult=lr_mult,\n",
    "            project=PROJECT_ROOT,\n",
    "            epochs=EPOCHS,\n",
    "        )\n",
    "        all_runs.append(name)\n",
    "\n",
    "# --- Batch sweep at default LR (1x) ---\n",
    "for model_ckpt in MODELS:\n",
    "    for b in BATCHES:\n",
    "        name = train_one(\n",
    "            model_ckpt=model_ckpt,\n",
    "            imgsz=IMGSZ,\n",
    "            batch=b,\n",
    "            lr_mult=1.0,\n",
    "            project=PROJECT_ROOT,\n",
    "            epochs=EPOCHS,\n",
    "        )\n",
    "        all_runs.append(name)\n",
    "\n",
    "print(\"\\nFinished / queued runs:\")\n",
    "for r in all_runs:\n",
    "    print(\" -\", r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f50fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate columns (Ultralytics results.csv naming varies slightly by version)\n",
    "COL_VAL_LOSS = [\n",
    "    \"val/box_loss\", \"metrics/val_box_loss\", \"val/box_loss\"\n",
    "]\n",
    "COL_MAP5095 = [\n",
    "    \"metrics/mAP50-95(B)\", \"metrics/mAP50-95\", \"metrics/mAP50-95(B)\", \"metrics/mAP50-95\"\n",
    "]\n",
    "COL_MAP50 = [\n",
    "    \"metrics/mAP50(B)\", \"metrics/mAP50\", \"metrics/mAP50(B)\", \"metrics/mAP50\"\n",
    "]\n",
    "COL_PREC = [\n",
    "    \"metrics/precision(B)\", \"metrics/precision\", \"metrics/precision(B)\"\n",
    "]\n",
    "COL_RECALL = [\n",
    "    \"metrics/recall(B)\", \"metrics/recall\", \"metrics/recall(B)\"\n",
    "]\n",
    "COL_TRAIN_BOX = [\"train/box_loss\"]\n",
    "COL_TRAIN_CLS = [\"train/cls_loss\"]\n",
    "COL_TRAIN_DFL = [\"train/dfl_loss\"]\n",
    "\n",
    "def plot_required_for_run(project: str, name: str):\n",
    "    df = load_results_csv(project, name)\n",
    "\n",
    "    # Filter epochs 1..15 (already should be 15, but keeps it explicit)\n",
    "    df = df[(df[\"epoch_1based\"] >= 1) & (df[\"epoch_1based\"] <= 15)].copy()\n",
    "\n",
    "    val_box_col = pick_col(df, COL_VAL_LOSS)\n",
    "    map5095_col = pick_col(df, COL_MAP5095)\n",
    "    map50_col   = pick_col(df, COL_MAP50)\n",
    "    prec_col    = pick_col(df, COL_PREC)\n",
    "    rec_col     = pick_col(df, COL_RECALL)\n",
    "    train_box   = pick_col(df, COL_TRAIN_BOX)\n",
    "    train_cls   = pick_col(df, COL_TRAIN_CLS)\n",
    "    train_dfl   = pick_col(df, COL_TRAIN_DFL)\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Columns found:\",\n",
    "          {\"val_box\": val_box_col, \"mAP50-95\": map5095_col, \"mAP50\": map50_col,\n",
    "           \"prec\": prec_col, \"recall\": rec_col,\n",
    "           \"train_box\": train_box, \"train_cls\": train_cls, \"train_dfl\": train_dfl})\n",
    "\n",
    "    # 1) Validation loss vs epochs\n",
    "    if val_box_col:\n",
    "        plt.figure()\n",
    "        plot_metric(df, val_box_col, f\"{name}: Validation box_loss vs Epochs\", label=\"val box_loss\")\n",
    "        finalize_plot(\"Loss\")\n",
    "    else:\n",
    "        print(\"Could not find a validation loss column for this run.\")\n",
    "\n",
    "    # 2) mAP50–95 vs epochs\n",
    "    if map5095_col:\n",
    "        plt.figure()\n",
    "        plot_metric(df, map5095_col, f\"{name}: mAP50-95 vs Epochs\", label=\"mAP50-95\")\n",
    "        finalize_plot(\"mAP50-95\")\n",
    "    else:\n",
    "        print(\"Could not find mAP50-95 column for this run.\")\n",
    "\n",
    "    # 3) box_loss, cls_loss, dfl_loss, Precision, Recall, mAP50 vs epochs\n",
    "    plt.figure()\n",
    "    did_any = False\n",
    "    for col, lab in [(train_box, \"train box_loss\"), (train_cls, \"train cls_loss\"), (train_dfl, \"train dfl_loss\")]:\n",
    "        if col:\n",
    "            plt.plot(df[\"epoch_1based\"], df[col], marker=\"o\", linewidth=1.5, label=lab)\n",
    "            did_any = True\n",
    "    if did_any:\n",
    "        plt.title(f\"{name}: Train losses vs Epochs\")\n",
    "        plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.grid(True, alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()\n",
    "    else:\n",
    "        print(\"Could not find train losses (box/cls/dfl) columns.\")\n",
    "\n",
    "    plt.figure()\n",
    "    did_any = False\n",
    "    for col, lab in [(prec_col, \"Precision\"), (rec_col, \"Recall\"), (map50_col, \"mAP50\")]:\n",
    "        if col:\n",
    "            plt.plot(df[\"epoch_1based\"], df[col], marker=\"o\", linewidth=1.5, label=lab)\n",
    "            did_any = True\n",
    "    if did_any:\n",
    "        plt.title(f\"{name}: Precision / Recall / mAP50 vs Epochs\")\n",
    "        plt.xlabel(\"Epoch\"); plt.ylabel(\"Metric\"); plt.grid(True, alpha=0.3); plt.legend(); plt.tight_layout(); plt.show()\n",
    "    else:\n",
    "        print(\"Could not find Precision/Recall/mAP50 columns.\")\n",
    "\n",
    "# Run for all experiment folders we made\n",
    "for name in all_runs:\n",
    "    # de-dup names if any repeats\n",
    "    pass\n",
    "\n",
    "for name in sorted(set(all_runs)):\n",
    "    plot_required_for_run(PROJECT_ROOT, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_runs(project: str, run_names: list, title: str):\n",
    "    # Load dfs\n",
    "    dfs = {}\n",
    "    for rn in run_names:\n",
    "        df = load_results_csv(project, rn)\n",
    "        df = df[(df[\"epoch_1based\"] >= 1) & (df[\"epoch_1based\"] <= 15)].copy()\n",
    "        dfs[rn] = df\n",
    "\n",
    "    # Determine columns from the first df that has them\n",
    "    any_df = next(iter(dfs.values()))\n",
    "    val_box_col = pick_col(any_df, COL_VAL_LOSS) or pick_col(any_df, [\"val/box_loss\", \"metrics/val_box_loss\"])\n",
    "    map5095_col = pick_col(any_df, COL_MAP5095)\n",
    "    map50_col   = pick_col(any_df, COL_MAP50)\n",
    "\n",
    "    # Validation loss comparison\n",
    "    if val_box_col:\n",
    "        plt.figure()\n",
    "        for rn, df in dfs.items():\n",
    "            if val_box_col in df.columns:\n",
    "                plot_metric(df, val_box_col, f\"{title}: Validation Loss\", label=rn)\n",
    "        finalize_plot(\"Val loss\")\n",
    "    else:\n",
    "        print(f\"[{title}] Could not find a validation loss column.\")\n",
    "\n",
    "    # Accuracy comparison (mAP50-95)\n",
    "    if map5095_col:\n",
    "        plt.figure()\n",
    "        for rn, df in dfs.items():\n",
    "            if map5095_col in df.columns:\n",
    "                plot_metric(df, map5095_col, f\"{title}: mAP50-95\", label=rn)\n",
    "        finalize_plot(\"mAP50-95\")\n",
    "    else:\n",
    "        print(f\"[{title}] Could not find mAP50-95 column.\")\n",
    "\n",
    "    # Optional: mAP50 comparison\n",
    "    if map50_col:\n",
    "        plt.figure()\n",
    "        for rn, df in dfs.items():\n",
    "            if map50_col in df.columns:\n",
    "                plot_metric(df, map50_col, f\"{title}: mAP50\", label=rn)\n",
    "        finalize_plot(\"mAP50\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34640e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR sweep groups: for each model, compare lr1x vs lr5x vs lr0.2x with batch fixed at 16\n",
    "for model_ckpt in MODELS:\n",
    "    base = Path(model_ckpt).stem\n",
    "    lr_group = [\n",
    "        run_name(model_ckpt, IMGSZ, 16, 1.0),\n",
    "        run_name(model_ckpt, IMGSZ, 16, 5.0),\n",
    "        run_name(model_ckpt, IMGSZ, 16, 0.2),\n",
    "    ]\n",
    "    compare_runs(PROJECT_ROOT, lr_group, title=f\"{base} LR Sweep (batch=16)\")\n",
    "\n",
    "# Batch sweep groups: for each model, compare b8 vs b16 vs b32 with lr fixed at 1x\n",
    "for model_ckpt in MODELS:\n",
    "    base = Path(model_ckpt).stem\n",
    "    batch_group = [\n",
    "        run_name(model_ckpt, IMGSZ, 8, 1.0),\n",
    "        run_name(model_ckpt, IMGSZ, 16, 1.0),\n",
    "        run_name(model_ckpt, IMGSZ, 32, 1.0),\n",
    "    ]\n",
    "    compare_runs(PROJECT_ROOT, batch_group, title=f\"{base} Batch Sweep (lr=1x)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ricky_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
